
from typing import Dict, List, Optional, Any
import json
import re
from langchain_community.chat_models import ChatOpenAI
import logging
from loguru import logger

async def get_llm(model_name: str = None, temperature: float = None, max_tokens: int = None, top_p: float = None):
    """Initializes and returns the appropriate language model client.
    
    This function selects the API provider based on the PREFERRED_API_PROVIDER setting.
    It handles model name prefixes to ensure compatibility with both providers.
    
    When using OpenRouter, you can specify models from different providers:
    - OpenAI models: "openai/gpt-4", "openai/gpt-3.5-turbo", or just "gpt-4" (prefix added automatically)
    - Google models: "google/gemini-pro"
    - Anthropic models: "anthropic/claude-2"
    - Meta models: "meta-llama/llama-2-70b-chat"
    
    When using OpenAI directly, provider prefixes are automatically removed.
    """
    from app.services.config_service import ConfigService
    
    # Get dynamic configuration
    config_service = ConfigService()
    await config_service._load_config()
    llm_settings = await config_service.get_llm_settings()
    
    # Determine which API provider to use based on configuration
    preferred_provider = llm_settings.preferred_api_provider.lower()
    
    # Check if we have the necessary API keys
    has_openai_key = bool(llm_settings.openai_api_key)
    has_openrouter_key = bool(llm_settings.openrouter_api_key)
    
    # Determine which provider to use based on preference and available keys
    use_openrouter = False
    
    if preferred_provider == "auto":
        # In auto mode, use OpenRouter if available, otherwise fall back to OpenAI
        use_openrouter = has_openrouter_key
    elif preferred_provider == "openrouter":
        # Explicitly use OpenRouter
        if has_openrouter_key:
            use_openrouter = True
        else:
            print("OpenRouter is preferred but API key is not set. Falling back to OpenAI if available.")
            use_openrouter = False
    elif preferred_provider == "openai":
        # Explicitly use OpenAI
        use_openrouter = False
    else:
        logging.warning(f"Unknown provider preference '{preferred_provider}'. Using auto selection.")
        use_openrouter = has_openrouter_key
    
    # Use the selected provider
    if use_openrouter and has_openrouter_key:
        # Using OpenRouter
        print(f"Using OpenRouter API with model: {model_name or llm_settings.default_model}")
        
        # Make sure the model name is properly formatted for OpenRouter
        # OpenRouter model names should include the provider prefix (e.g., google/gemini-pro, anthropic/claude-2)
        selected_model = model_name or llm_settings.default_model
        
        # Ensure the model has a provider prefix
        if '/' not in selected_model:
            # If no provider prefix, assume it's an OpenAI model
            selected_model = f"openai/{selected_model}"
            logging.info(f"Added default 'openai/' prefix to model name: {selected_model}")
        
        # Create the ChatOpenAI instance with OpenRouter configuration
        return ChatOpenAI(
            model_name=selected_model,
            temperature= llm_settings.temperature,
            max_tokens= llm_settings.max_tokens,
            openai_api_key=llm_settings.openrouter_api_key,
            openai_api_base=llm_settings.openrouter_api_base,
        )
    elif has_openai_key:
        # Using OpenAI directly
        logging.info(f"Using OpenAI API with model: {model_name or llm_settings.default_model}")
        
        # For OpenAI, we need to remove any provider prefix if present
        selected_model = model_name or llm_settings.default_model
        if selected_model.startswith("openai/"):
            selected_model = selected_model.replace("openai/", "")
        
        return ChatOpenAI(
            model_name=selected_model,
            temperature=llm_settings.temperature,
            max_tokens=llm_settings.max_tokens,
            openai_api_key=llm_settings.openai_api_key,
        )
    else:
        raise ValueError("Either OPENAI_API_KEY or OPENROUTER_API_KEY must be set")
from langchain_core.messages import HumanMessage, AIMessage, SystemMessage
from langchain.memory import ConversationBufferMemory
from langchain.chains import ConversationChain

from app.core.config import settings
from app.schemas.chat import ChatMessage
from app.services.knowledge_base import get_knowledge_base_service
from app.services.config_service import ConfigService


class ChatService:
    """Service for managing chat interactions with various language models.
    
    This service is responsible for handling chat sessions, maintaining conversation
    history, and interacting with language models via LangChain. It supports:
    
    - OpenAI models directly through the OpenAI API
    - Multiple model providers (OpenAI, Google, Anthropic, Meta, etc.) through OpenRouter
    
    The provider selection is controlled by the PREFERRED_API_PROVIDER setting.
    """
    
    def __init__(self):
        """Initialize the chat service."""
        self._sessions: Dict[str, ConversationChain] = {}
        self._memories: Dict[str, ConversationBufferMemory] = {}
        self.config_service = ConfigService()
        self.generalAnswer = False
        self._config_updated_at: Optional[str] = None
        # Note: API key validation is now done dynamically in get_llm function
        

    
    async def _get_or_create_session(self, user_id: str, model: str = None, parameters: dict = None) -> ConversationChain:
        """Get an existing chat session or create a new one.
        
        Args:
            user_id: Unique identifier for the user session
            
        Returns:
            A LangChain ConversationChain for the user
        """
        await self._ensure_latest_config()
        if user_id not in self._sessions:
            # Get dynamic configuration for system prompt
            await self.config_service._load_config()
            rag_settings = await self.config_service.get_rag_settings()
            
            # Create a new memory for this user
            memory = ConversationBufferMemory(return_messages=True)
            self._memories[user_id] = memory
            
            # Create a new chat model with the configured settings
            params = parameters or {}
            llm = await get_llm()
            
            # Create a conversation chain with the memory
            self._sessions[user_id] = ConversationChain(
                llm=llm,
                memory=memory,
                verbose=False
            )
            
            # Add system prompt to establish model behavior for general knowledge responses
            system_prompt = rag_settings.system_prompt
            self._sessions[user_id].memory.chat_memory.add_message(SystemMessage(content=system_prompt))
        
        return self._sessions[user_id]

    async def _ensure_latest_config(self) -> None:
        await self.config_service._load_config()
        cfg = await self.config_service.get_config()
        ts = cfg.updated_at or cfg.created_at
        if ts != self._config_updated_at:
            self._sessions.clear()
            self._memories.clear()
            self._config_updated_at = ts

    async def refresh(self) -> None:
        await self.config_service._load_config()
        cfg = await self.config_service.get_config()
        self._sessions.clear()
        self._memories.clear()
        self._config_updated_at = cfg.updated_at or cfg.created_at
    
    def _is_topic_related_to_domain(self, query: str) -> bool:
        """Check if the query is related to the knowledge base domain.
        
        Args:
            query: The user's question
            
        Returns:
            True if the query is related to the domain, False otherwise
        """
        query_lower = query.lower()
        
        # Strongly unrelated topics that should be referred to humans
        # These are topics completely outside PersianWay's domain
        strongly_unrelated_keywords = [
            # Ø³ÛŒØ§Ø³Øª Ùˆ Ø­Ú©ÙˆÙ…Øª (Politics & Government)
            'Ø³ÛŒØ§Ø³Øª', 'Ø§Ù†ØªØ®Ø§Ø¨Ø§Øª', 'Ø¯ÙˆÙ„Øª', 'Ù…Ø¬Ù„Ø³', 'Ø±Ø¦ÛŒØ³ Ø¬Ù…Ù‡ÙˆØ±', 'ÙˆØ²ÛŒØ±', 'Ø­Ø²Ø¨',
            'Ø³ÛŒØ§Ø³ØªÙ…Ø¯Ø§Ø±', 'Ø±Ø§ÛŒ', 'Ú©Ø§Ù†Ø¯ÛŒØ¯Ø§', 'Ú©Ø§Ø¨ÛŒÙ†Ù‡', 'Ù¾Ø§Ø±Ù„Ù…Ø§Ù†', 'Ù‚Ø§Ù†ÙˆÙ†', 'Ù‚Ø¶Ø§ÙˆØª',
            'Ø¯Ø§Ø¯Ú¯Ø§Ù‡', 'ÙˆÚ©ÛŒÙ„', 'Ù‚Ø§Ø¶ÛŒ', 'Ø¬Ø±Ù…', 'Ù…Ø¬Ø§Ø²Ø§Øª', 'Ø²Ù†Ø¯Ø§Ù†', 'Ù¾Ù„ÛŒØ³','Ø¬Ù†Ú¯',
            'Ø³ÙÛŒØ±', 'Ø¯ÛŒÙ¾Ù„Ù…Ø§Øª', 'Ø³ÙØ§Ø±Øª', 'Ú©Ù†Ø³ÙˆÙ„Ú¯Ø±ÛŒ','Ø³Ø§Ø²Ù…Ø§Ù† Ù…Ù„Ù„',
            'Ù†Ø§ØªÙˆ', 'Ø§ØªØ­Ø§Ø¯ÛŒÙ‡ Ø§Ø±ÙˆÙ¾Ø§', 'Ø³Ù†Ø§', 'Ú©Ù†Ú¯Ø±Ù‡', 'Ù…Ø°Ø§Ú©Ø±Ù‡', 'ØªØ­Ø±ÛŒÙ…', 'Ù…Ø¹Ø§Ù‡Ø¯Ù‡',
            'Ø§Ø³ØªÛŒØ¶Ø§Ø­', 'ÙØ³Ø§Ø¯', 'Ø±Ø´ÙˆÙ‡', 'Ø§Ø®ØªÙ„Ø§Ø³', 'Ø¨Ø±Ø§Ù†Ø¯Ø§Ø²ÛŒ', 'Ú©ÙˆØ¯ØªØ§', 'Ø§Ù†Ù‚Ù„Ø§Ø¨',
            'ØªØ¸Ø§Ù‡Ø±Ø§Øª', 'Ø§Ø¹ØªØµØ§Ø¨', 'Ø­Ù‚ÙˆÙ‚ Ø¨Ø´Ø±', 'Ø³Ø§Ù†Ø³ÙˆØ±',
            'politics', 'election', 'government', 'parliament', 'president', 'minister',
            'party', 'politician', 'vote', 'candidate', 'cabinet', 'law', 'court',
            'lawyer', 'judge', 'crime', 'punishment', 'prison', 'police', 'america',
            'usa', 'iran', 'china', 'russia', 'europe', 'country', 'nation', 'diplomacy',
            'ambassador', 'diplomat', 'embassy', 'consulate', 'representative', 'un',
            'nato', 'eu', 'senate', 'congress', 'negotiation', 'sanction', 'treaty',
            'impeachment', 'corruption', 'bribery', 'embezzlement', 'coup', 'revolution',
            'protest', 'demonstration', 'strike', 'human rights', 'freedom of speech', 'censorship',
            'democracy', 'dictatorship', 'monarchy', 'republic', 'constitution', 'referendum',
            'propaganda', 'military', 'army', 'navy', 'air force', 'defense', 'nuclear',
            'terrorism', 'extremism', 'intelligence', 'spy', 'security council',
            
            # ÙˆØ±Ø²Ø´ (Sports)
            'ÙÙˆØªØ¨Ø§Ù„', 'ÙˆØ§Ù„ÛŒØ¨Ø§Ù„', 'Ø¨Ø³Ú©ØªØ¨Ø§Ù„', 'ØªÙ†ÛŒØ³', 'Ø´Ù†Ø§', 'Ø¯ÙˆÚ†Ø±Ø®Ù‡ Ø³ÙˆØ§Ø±ÛŒ',
            'Ú©ÙˆÙ‡Ù†ÙˆØ±Ø¯ÛŒ', 'Ø§Ø³Ú©ÛŒ', 'Ú©Ø´ØªÛŒ', 'Ø¬ÙˆØ¯Ùˆ', 'Ú©Ø§Ø±Ø§ØªÙ‡', 'ØªÚ©ÙˆØ§Ù†Ø¯Ùˆ', 'Ø¨ÙˆÚ©Ø³',
          'Ø¨Ø§Ø²ÛŒÚ©Ù†', 'Ø§Ø³ØªØ§Ø¯ÛŒÙˆÙ…', 'Ù…Ø³Ø§Ø¨Ù‚Ù‡', 'Ù‚Ù‡Ø±Ù…Ø§Ù†ÛŒ',
            'Ø§Ù„Ù…Ù¾ÛŒÚ©', 'Ø¬Ø§Ù… Ø¬Ù‡Ø§Ù†ÛŒ', 'Ù„ÛŒÚ¯', 'ÙÛŒÙ†Ø§Ù„',
            'football', 'volleyball', 'basketball', 'tennis', 'swimming', 'cycling',
            'mountaineering', 'skiing', 'wrestling', 'judo', 'karate', 'taekwondo',
            'boxing', 'sport', 'team', 'player', 'coach', 'stadium', 'competition',
            'championship', 'olympics', 'world cup', 'league', 'final', 'goal', 'score',
            
            # Ø³Ø±Ú¯Ø±Ù…ÛŒ Ùˆ Ù‡Ù†Ø± (Entertainment & Arts)
             'Ø³ÛŒÙ†Ù…Ø§', 'Ø¨Ø§Ø²ÛŒÚ¯Ø±', 'Ú©Ø§Ø±Ú¯Ø±Ø¯Ø§Ù†', 'ØªÙ„ÙˆÛŒØ²ÛŒÙˆÙ†',
            'Ù…ÙˆØ³ÛŒÙ‚ÛŒ', 'Ø®ÙˆØ§Ù†Ù†Ø¯Ù‡', 'Ø¢Ù‡Ù†Ú¯', 'Ú©Ù†Ø³Ø±Øª', 'Ø¢Ù„Ø¨ÙˆÙ…', 'Ù¾ÛŒØ§Ù†Ùˆ', 'Ú¯ÛŒØªØ§Ø±',
            'Ù†Ù‚Ø§Ø´ÛŒ', 'Ù…Ø¬Ø³Ù…Ù‡ Ø³Ø§Ø²ÛŒ', 'Ø¹Ú©Ø§Ø³ÛŒ', 'ØªØ¦Ø§ØªØ±', 'Ø±Ù‚Øµ', 'Ø¨Ø§Ù„Ù‡', 'Ø§Ù¾Ø±Ø§',
            'Ø±Ù…Ø§Ù†', 'Ø´Ø¹Ø±', 'Ù†ÙˆÛŒØ³Ù†Ø¯Ù‡', 'Ø´Ø§Ø¹Ø±', 'Ø§Ø¯Ø¨ÛŒØ§Øª',
            'movie', 'cinema', 'actor', 'director', 'television', 'series', 'program',
            'music', 'singer', 'song', 'concert', 'album', 'instrument', 'piano',
            'guitar', 'painting', 'sculpture', 'photography', 'theater', 'dance',
            'ballet', 'opera', 'book', 'novel', 'poetry', 'writer', 'poet',
            'literature', 'story',
            
            # ÙÙ†Ø§ÙˆØ±ÛŒ Ùˆ Ø§Ù„Ú©ØªØ±ÙˆÙ†ÛŒÚ© (Technology & Electronics)
            'Ú©Ø§Ù…Ù¾ÛŒÙˆØªØ±', 'Ù„Ù¾ ØªØ§Ù¾', 'Ù…ÙˆØ¨Ø§ÛŒÙ„', 'ØªØ¨Ù„Øª', 'Ù†Ø±Ù… Ø§ÙØ²Ø§Ø±', 'Ø¨Ø±Ù†Ø§Ù…Ù‡ Ù†ÙˆÛŒØ³ÛŒ',
            'Ø§Ù¾Ù„ÛŒÚ©ÛŒØ´Ù†', 'ÙˆØ¨ Ø³Ø§ÛŒØª', 'Ø§ÛŒÙ†ØªØ±Ù†Øª', 'Ø¯ÛŒØªØ§Ø¨ÛŒØ³',
             'Ø¨Ù„Ø§Ú© Ú†ÛŒÙ†', 'Ø§Ø±Ø² Ø¯ÛŒØ¬ÛŒØªØ§Ù„', 'Ø¨ÛŒØª Ú©ÙˆÛŒÙ†',
             'Ú©Ù†Ø³ÙˆÙ„', 'Ù¾Ù„ÛŒ Ø§Ø³ØªÛŒØ´Ù†', 'Ø§ÛŒÚ©Ø³ Ø¨Ø§Ú©Ø³', 'Ù†ÛŒÙ†ØªÙ†Ø¯Ùˆ',
            'computer', 'laptop', 'mobile', 'tablet', 'software', 'programming',
            'application', 'website', 'internet', 'network', 'server', 'database',
            'artificial intelligence', 'robot', 'blockchain', 'cryptocurrency',
            'bitcoin', 'game', 'gaming', 'console', 'playstation', 'xbox', 'nintendo',
            
           
            # Ø§Ù…Ù„Ø§Ú© Ùˆ Ù…Ø³Ú©Ù† (Real Estate & Housing)
            'Ø¢Ù¾Ø§Ø±ØªÙ…Ø§Ù†', 'ÙˆÛŒÙ„Ø§',
            'Ø±Ù‡Ù†', 'ÙˆØ¯ÛŒØ¹Ù‡', 'Ù…Ø´Ø§ÙˆØ± Ø§Ù…Ù„Ø§Ú©', 'Ù‚ÛŒÙ…Øª Ù…Ø³Ú©Ù†', 'Ù…ØªØ±Ø§Ú˜'
            
            'house', 'apartment', 'villa', 'land', 'building', 'rent', 'buy',
            'sell', 'mortgage', 'deposit', 'real estate agent', 'housing price',
            'area', 'room', 'kitchen', 'bathroom', 'parking', 'storage', 'balcony',
            
            # Ù…Ø§Ù„ÛŒ Ùˆ Ø¨Ø§Ù†Ú©ÛŒ (Finance & Banking)
             'ÙˆØ§Ù…', 'Ø³Ù¾Ø±Ø¯Ù‡', 'Ø³ÙˆØ¯', 'Ø¨Ù‡Ø±Ù‡', 'Ú†Ú©', 'Ú©Ø§Ø±Øª Ø§Ø¹ØªØ¨Ø§Ø±ÛŒ'
            , 'Ø¯Ù„Ø§Ø±', 'ÛŒÙˆØ±Ùˆ', 'Ø¨ÙˆØ±Ø³', 'Ø³Ù‡Ø§Ù…', 'Ø³Ø±Ù…Ø§ÛŒÙ‡ Ú¯Ø°Ø§Ø±ÛŒ',
            'Ø¨ÛŒÙ…Ù‡', 'Ù…Ø§Ù„ÛŒØ§Øª', 'Ø­Ø³Ø§Ø¨Ø¯Ø§Ø±ÛŒ', 'Ø§Ù‚ØªØµØ§Ø¯',
            'money', 'bank', 'investment', 'stock', 'economy', 'financial', 'accounting',
            'loan', 'deposit', 'profit', 'interest', 'check', 'credit card',
            'account', 'currency', 'dollar', 'euro', 'stock market',
            'shares', 'insurance', 'tax', 'inflation', 'recession',
            
            # Ø¢Ù…ÙˆØ²Ø´ Ùˆ ØªØ­ØµÛŒÙ„ (Education)
            'Ø¯Ø§Ù†Ø´Ú¯Ø§Ù‡'
            , 'Ø¯ÛŒÙ¾Ù„Ù…', 'Ù„ÛŒØ³Ø§Ù†Ø³', 'ÙÙˆÙ‚ Ù„ÛŒØ³Ø§Ù†Ø³', 'Ø¯Ú©ØªØ±Ø§',
            'Ø±ÛŒØ§Ø¶ÛŒ', 'ÙÛŒØ²ÛŒÚ©', 'Ø´ÛŒÙ…ÛŒ', 'Ø²ÛŒØ³Øª Ø´Ù†Ø§Ø³ÛŒ', 'Ø¬ØºØ±Ø§ÙÛŒØ§',
            'university', 'school', 'class', 'teacher', 'professor', 'student',
            'exam', 'grade', 'certificate', 'diploma', 'bachelor', 'master',
            'phd', 'mathematics', 'physics', 'chemistry', 'biology', 'history',
            'geography'
        ]
        
        # Only check for strongly unrelated topics
        # Return False only if the query contains strongly unrelated keywords
        for keyword in strongly_unrelated_keywords:
            # Use word boundary check to avoid matching substrings within words
            # For example, to avoid matching 'Ø±Ø§ÛŒ' in 'Ø¨Ø±Ø§ÛŒ'
            # Create a pattern with word boundaries
            pattern = r'\b' + re.escape(keyword) + r'\b'
            if re.search(pattern, query_lower):
                # Log the keyword that caused the rejection
                logger.info(f"Query rejected due to unrelated keyword: '{keyword}' found in query: '{query}'")
                # You can access this log in your application logs
                return False, keyword
            
        # For all other queries, assume they are related to the domain
        return True, None



    async def generate_conversation_title(self, message: str) -> str:
        """Generate a conversation title based on the user's message.
        
        Args:
            message: The user's message to generate a title from
            
        Returns:
            A concise title for the conversation
        """
        try:
            # Get LLM instance
            llm = await get_llm(model_name="gpt-4o-mini",temperature=0.1, max_tokens=100)
            
            # Create a prompt to generate a concise title
            title_prompt = f"""Based on the following user message, generate a concise and descriptive title (maximum 5-7 words) for this conversation. The title should be in the same language as the user's message.

User message: {message}

Title:"""
            
            # Generate title using the LLM
            response = await llm.ainvoke([HumanMessage(content=title_prompt)])
            
            # Extract and clean the title
            title = response.content.strip()
            
            # Remove quotes if present
            if title.startswith('"') and title.endswith('"'):
                title = title[1:-1]
            if title.startswith("'") and title.endswith("'"):
                title = title[1:-1]
                
            # Limit title length as a safety measure
            if len(title) > 100:
                title = title[:97] + "..."
                
            return title
            
        except Exception as e:
            logger.error(f"Error generating conversation title: {str(e)}")
            # Return a default title if generation fails
            return "New Conversation"

    async def detect_public_data_intent(
        self,
        message: str,
        conversation_history: Optional[Any] = None,
        *,
        llm: Optional[Any] = None
    ) -> bool:
        """Legacy method for backward compatibility.
        
        Detects whether the user's message is about PersianWay public data.
        This is now a wrapper around detect_query_intent.
        
        Args:
            message: The latest user message.
            conversation_history: Prior conversation exchanges.
            llm: Optional pre-configured LLM instance.
        
        Returns:
            True if the intent relates to public PersianWay company information, otherwise False.
        """
        result = await self.detect_query_intent(message, conversation_history, llm=llm)
        return result.get("is_public", False)
    
    async def detect_query_intent(
        self,
        message: str,
        conversation_history: Optional[Any] = None,
        *,
        llm: Optional[Any] = None
    ) -> Dict[str, Any]:
        """Detect the intent of the user's query.
        
        Args:
            message: The latest user message.
            conversation_history: Prior conversation exchanges (can be ConversationResponse, list of messages, or None).
            llm: Optional pre-configured LLM instance (used mainly for testing).
        
        Returns:
            A dictionary with:
                - intent: One of "PUBLIC", "PRIVATE", or "OFF_TOPIC"
                - is_public: Boolean indicating if it's a public query (for backward compatibility)
                - explanation: Reason for the classification
                - off_topic_message: Optional message to redirect user (for OFF_TOPIC)
        """
        if not message or not message.strip():
            return {
                "intent": "NEEDS_CLARIFICATION",
                "is_public": False,
                "explanation": "Empty message",
                "clarification_prompt": "Ù„Ø·ÙØ§Ù‹ Ø³ÙˆØ§Ù„ ÛŒØ§ Ø¯Ø±Ø®ÙˆØ§Ø³Øª Ø®ÙˆØ¯ Ø±Ø§ Ù…Ø´Ø®Øµ Ú©Ù†ÛŒØ¯."
            }

        formatted_history: List[str] = []
        
        if conversation_history:
            messages_to_process = []
            
            # Handle ConversationResponse object
            if hasattr(conversation_history, 'messages'):
                messages_to_process = conversation_history.messages[-6:]  # Last 6 messages
            # Handle list of ConversationResponse objects
            elif isinstance(conversation_history, list) and conversation_history:
                if hasattr(conversation_history[0], 'messages'):
                    # It's a list of ConversationResponse, take the last one
                    messages_to_process = conversation_history[-1].messages[-6:]
                else:
                    # It's already a list of messages
                    messages_to_process = conversation_history[-6:]
            
            # Process messages
            for entry in messages_to_process:
                role = None
                content = None

                # Handle MessageResponse objects (from ConversationResponse)
                if hasattr(entry, 'role') and hasattr(entry, 'content'):
                    role = entry.role
                    content = entry.content
                # Handle ChatMessage objects
                elif isinstance(entry, ChatMessage):
                    role = entry.role
                    content = entry.content
                # Handle dict
                elif isinstance(entry, dict):
                    role = entry.get("role")
                    content = entry.get("content")

                if role and content:
                    formatted_history.append(f"{role}: {content}")
        
        # Log the extracted history for debugging
        if formatted_history:
            logger.debug(f"Intent detection extracted {len(formatted_history)} messages from conversation history")
        
        history_block = "\n".join(formatted_history) if formatted_history else "No prior conversation."

        classifier_prompt = (
    "You are an intent classifier for PersianWay (Ù¾Ø±Ø´ÛŒÙ† ÙˆÛŒ) customer support.\n\n"
    
    "PersianWay is a Network Marketing company operating under Iranian MLM regulations.\n"
    "The company has THREE main product areas: Agriculture, Health, and Beauty.\n\n"
    
    "ðŸ“‹ CLASSIFICATION CATEGORIES:\n"
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
    
    "1ï¸âƒ£ PUBLIC (Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø¹Ù…ÙˆÙ…ÛŒ Ø´Ø±Ú©Øª)\n"
    "   ALL questions about the company, its operations, and network marketing business:\n"
    "   \n"
    "   ðŸ¢ Company Information:\n"
    "   â€¢ Company history, establishment, licenses\n"
    "   â€¢ Mission, vision, values, brand information\n"
    "   â€¢ Office locations, contact details, addresses\n"
    "   â€¢ Organizational structure, management team\n"
    "   â€¢ All brands (Hapix, Celux, Frei Ã–l, Magical, Dream World)\n"
    "   \n"
    "   ðŸ’¼ Network Marketing Business Operations:\n"
    "   â€¢ Membership & Registration (Ø«Ø¨Øªâ€ŒÙ†Ø§Ù…ØŒ Ø¹Ø¶ÙˆÛŒØªØŒ Ø¬Ø§ÛŒÚ¯Ø§Ù‡)\n"
    "   â€¢ Commission & Compensation (Ù¾ÙˆØ±Ø³Ø§Ù†ØªØŒ Ø¯Ø±Ø¢Ù…Ø¯ØŒ Ù¾Ø§Ø¯Ø§Ø´)\n"
    "   â€¢ Violations & Penalties (ØªØ®Ù„ÙØ§ØªØŒ Ù…Ø¬Ø§Ø²Ø§ØªØŒ Ø§Ø®Ø·Ø§Ø±)\n"
    "   â€¢ License & Permits (Ù¾Ø±ÙˆØ§Ù†Ù‡ Ú©Ø³Ø¨ØŒ Ù…Ø¬ÙˆØ² ÙØ¹Ø§Ù„ÛŒØª)\n"
    "   â€¢ Network Status (ÙØ¹Ø§Ù„/ØºÛŒØ±ÙØ¹Ø§Ù„ØŒ ØªØ¹Ù„ÛŒÙ‚ØŒ Ù†Ù…Ø§Ø¯)\n"
    "   â€¢ MLM Regulations (Ø¢ÛŒÛŒÙ†â€ŒÙ†Ø§Ù…Ù‡ØŒ Ù‚ÙˆØ§Ù†ÛŒÙ†ØŒ Ù…Ù‚Ø±Ø±Ø§Øª)\n"
    "   â€¢ Distributor Rights (Ø­Ù‚ÙˆÙ‚ Ù†Ù…Ø§ÛŒÙ†Ø¯Ú¯Ø§Ù†ØŒ Ù‚Ø±Ø§Ø±Ø¯Ø§Ø¯Ù‡Ø§)\n"
    "   â€¢ Returns & Refunds (Ù…Ø±Ø¬ÙˆØ¹ÛŒØŒ Ø§Ø³ØªØ±Ø¯Ø§Ø¯ ÙˆØ¬Ù‡)\n"
    "   â€¢ Invoicing & Documentation (ÙØ§Ú©ØªÙˆØ±ØŒ Ø§Ø³Ù†Ø§Ø¯ Ù…Ø§Ù„ÛŒ)\n"
    "   â€¢ Training & Events (Ø¢Ù…ÙˆØ²Ø´ØŒ Ø±ÙˆÛŒØ¯Ø§Ø¯Ù‡Ø§ØŒ Ú©Ù„Ø§Ø³â€ŒÙ‡Ø§)\n"
    "   â€¢ Downline Management (Ø²ÛŒØ±Ù…Ø¬Ù…ÙˆØ¹Ù‡ØŒ Ú¯Ø±ÙˆÙ‡ØŒ ØªÛŒÙ…)\n"
    "   â€¢ Product pricing, ordering, shipping\n"
    "   â€¢ Company policies and procedures\n"
    "   \n"
    "   Examples:\n"
    "   âœ“ 'Ø´Ø±Ú©Øª Ù¾Ø±Ø´ÛŒÙ† ÙˆÛŒ Ú†ÛŒØ³ØªØŸ'\n"
    "   âœ“ 'Ø¯ÙØªØ± Ù…Ø±Ú©Ø²ÛŒ Ú©Ø¬Ø§Ø³ØªØŸ'\n"
    "   âœ“ 'Ø¨Ø±Ù†Ø¯Ù‡Ø§ÛŒ Ø´Ù…Ø§ Ú†ÛŒÙ‡ØŸ'\n"
    "   âœ“ 'Ú†Ø·ÙˆØ± Ø¹Ø¶Ùˆ Ø¨Ø´Ù…ØŸ'\n"
    "   âœ“ 'Ù¾ÙˆØ±Ø³Ø§Ù†Øª Ú†Ø·ÙˆØ± Ù…Ø­Ø§Ø³Ø¨Ù‡ Ù…ÛŒØ´Ù‡ØŸ'\n"
    "   âœ“ 'ØªØ®Ù„ÙØ§Øª Ùˆ Ù…Ø¬Ø§Ø²Ø§Øªâ€ŒÙ‡Ø§ Ú†ÛŒÙ‡ØŸ'\n"
    "   âœ“ 'Ø´Ø±Ø§ÛŒØ· ØºÛŒØ±ÙØ¹Ø§Ù„ Ø´Ø¯Ù† Ø¬Ø§ÛŒÚ¯Ø§Ù‡ØŸ'\n"
    "   âœ“ 'ÙˆØ¶Ø¹ÛŒØª Ù†Ù…Ø§Ø¯ Ø²Ø±Ø¯ ÛŒØ¹Ù†ÛŒ Ú†ÛŒØŸ'\n"
    "   âœ“ 'Ø´Ø±Ø§ÛŒØ· Ù…Ø±Ø¬ÙˆØ¹ Ú©Ø§Ù„Ø§ Ú†ÛŒØ³ØªØŸ'\n"
    "   âœ“ 'Ù…Ø­ØµÙˆÙ„Ø§Øª Ø´Ù…Ø§ Ú†ÛŒÙ‡ØŸ' (general product inquiry)\n"
    "   âœ“ 'Ú†Ø·ÙˆØ± Ø³ÙØ§Ø±Ø´ Ø«Ø¨Øª Ú©Ù†Ù…ØŸ'\n\n"
    
    "2ï¸âƒ£ PRIVATE (Ø³ÙˆØ§Ù„Ø§Øª ØªØ®ØµØµÛŒ)\n"
    "   ONLY specialized technical questions in these domains:\n"
    "   \n"
    "   ðŸŒ¾ Agriculture (Ú©Ø´Ø§ÙˆØ±Ø²ÛŒ):\n"
    "   â€¢ Technical farming questions\n"
    "   â€¢ Crop management, planting methods\n"
    "   â€¢ Fertilizer types, pesticide usage\n"
    "   â€¢ Soil management, irrigation techniques\n"
    "   â€¢ Pest control, disease prevention\n"
    "   â€¢ Agricultural equipment technical specs\n"
    "   \n"
    "   ðŸ’Š Health & Wellness (Ø³Ù„Ø§Ù…Øª):\n"
    "   â€¢ Medical conditions and treatments\n"
    "   â€¢ Supplement dosage and interactions\n"
    "   â€¢ Nutrition science and dietary advice\n"
    "   â€¢ Health concerns and diagnosis\n"
    "   â€¢ Vitamins and minerals technical info\n"
    "   \n"
    "   ðŸ’„ Beauty & Skincare (Ø²ÛŒØ¨Ø§ÛŒÛŒ Ùˆ Ø¯Ø±Ù…Ø§Ù† Ù¾ÙˆØ³Øª):\n"
    "   â€¢ Skincare routines and techniques\n"
    "   â€¢ Ingredient analysis and effects\n"
    "   â€¢ Skin conditions and treatments\n"
    "   â€¢ Cosmetic formulations\n"
    "   â€¢ Beauty therapy methods\n"
    "   \n"
    "   Examples:\n"
    "   âœ“ 'Ø¨Ù‡ØªØ±ÛŒÙ† Ú©ÙˆØ¯ Ø¨Ø±Ø§ÛŒ Ú¯Ù†Ø¯Ù… Ú†ÛŒÙ‡ØŸ' (agriculture)\n"
    "   âœ“ 'Ú†Ø·ÙˆØ± Ø®Ø§Ú© Ø±Ùˆ Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ø´Øª Ú©Ù†Ù…ØŸ' (agriculture)\n"
    "   âœ“ 'ÙˆÛŒØªØ§Ù…ÛŒÙ† D Ú†Ù‡ ÙØ§ÛŒØ¯Ù‡â€ŒØ§ÛŒ Ø¯Ø§Ø±Ù‡ØŸ' (health)\n"
    "   âœ“ 'Ø¨Ø±Ø§ÛŒ Ù¾ÙˆØ³Øª Ø®Ø´Ú© Ú†ÛŒ Ø¨Ø®ÙˆØ±Ù…ØŸ' (health/beauty)\n"
    "   âœ“ 'Ø±ÙˆØºÙ† Ø¢Ø±Ú¯Ø§Ù† Ø¨Ø±Ø§ÛŒ Ù…Ùˆ Ø®ÙˆØ¨Ù‡ØŸ' (beauty)\n"
    "   âœ— 'Ù…Ø­ØµÙˆÙ„ Ù‡Ù¾ÛŒÚ©Ø³ Ú†Ù†Ø¯Ù‡ØŸ' â†’ PUBLIC (pricing)\n"
    "   âœ— 'Ú©Ø¬Ø§ Ù…Ø­ØµÙˆÙ„Ø§ØªÙˆ Ø¨Ø®Ø±Ù…ØŸ' â†’ PUBLIC (ordering)\n\n"
    
    "3ï¸âƒ£ OFF_TOPIC (Ø®Ø§Ø±Ø¬ Ø§Ø² Ø­ÙˆØ²Ù‡)\n"
    "   Questions COMPLETELY unrelated to PersianWay or its domains:\n"
    "   \n"
    "   Examples:\n"
    "   âœ“ 'Ø¨Ù‡ØªØ±ÛŒÙ† ØªÛŒÙ… ÙÙˆØªØ¨Ø§Ù„ØŸ' (sports)\n"
    "   âœ“ 'Ú†Ø·ÙˆØ± Ø¨Ø±Ù†Ø§Ù…Ù‡â€ŒÙ†ÙˆÛŒØ³ÛŒ ÛŒØ§Ø¯ Ø¨Ú¯ÛŒØ±Ù…ØŸ' (programming)\n"
    "   âœ“ 'Ù‚ÛŒÙ…Øª Ø¯Ù„Ø§Ø± Ø§Ù…Ø±ÙˆØ²ØŸ' (forex)\n"
    "   âœ“ 'ÙÛŒÙ„Ù… Ø®ÙˆØ¨ Ù¾ÛŒØ´Ù†Ù‡Ø§Ø¯ Ø¨Ø¯Ù‡' (entertainment)\n"
    "   âœ“ 'Ø¢Ø¨â€ŒÙˆÙ‡ÙˆØ§ÛŒ ØªÙ‡Ø±Ø§Ù† Ú†Ø·ÙˆØ±Ù‡ØŸ' (weather)\n\n"
    
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"

    "4ï¸âƒ£ GREETING (Ø³Ù„Ø§Ù… Ùˆ Ø´Ø±ÙˆØ¹ Ù…Ú©Ø§Ù„Ù…Ù‡)\n"
    "   Simple greetings or pleasantries indicating the user starts a conversation:\n"
    "   Examples:\n"
    "   âœ“ 'Ø³Ù„Ø§Ù…'\n"
    "   âœ“ 'Ø¯Ø±ÙˆØ¯'\n"
    "   âœ“ 'Ø®Ø³ØªÙ‡ Ù†Ø¨Ø§Ø´ÛŒØ¯'\n"
    "   âœ“ 'Ø³Ù„Ø§Ù… ÙˆÙ‚Øª Ø¨Ø®ÛŒØ±'\n"
    "   âœ“ 'hello'\n"
    "   âœ“ 'hi'\n"
    "   âœ“ 'hey'\n\n"
    "5ï¸âƒ£ FAREWELL (Ø®Ø¯Ø§Ø­Ø§ÙØ¸ÛŒ)\n"
    "   Ending the conversation politely. Examples: 'Ø®Ø¯Ø§Ø­Ø§ÙØ¸', 'Ù…Ù…Ù†ÙˆÙ†ØŒ Ø±ÙˆØ²ØªÙˆÙ† Ø¨Ø®ÛŒØ±'\n\n"
    "6ï¸âƒ£ SMALL_TALK (Ú¯ÙØªâ€ŒÙˆÚ¯ÙˆÛŒ Ú©ÙˆØªØ§Ù‡)\n"
    "   Friendly small talk. Examples: 'Ø­Ø§Ù„Øª Ú†Ø·ÙˆØ±Ù‡ØŸ', 'Ø±ÙˆØ² Ø¨Ø®ÛŒØ±'\n\n"
  
    "ðŸŽ¯ DECISION FLOWCHART:\n"
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
    
    "Step 1: Is it about PersianWay company, MLM business, operations, products (general), or ordering?\n"
    "        â†’ YES: PUBLIC\n"
    "        â†’ NO: Go to Step 2\n\n"
    
    "Step 2: Is it a SPECIALIZED TECHNICAL question about agriculture, health, or beauty?\n"
    "        (NOT about product availability/pricing, but about technical usage/science)\n"
    "        â†’ YES: PRIVATE\n"
    "        â†’ NO: Go to Step 3\n\n"
    
    "Step 3: Is it COMPLETELY unrelated to PersianWay's business domains?\n"
    "        â†’ YES: OFF_TOPIC\n"
    "        â†’ UNSURE: PUBLIC (default to PUBLIC when uncertain)\n\n"
    
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
    
    "âš ï¸ CRITICAL RULES:\n"
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
    
    "âœ… Always PUBLIC:\n"
    "â€¢ Company info, history, locations, brands\n"
    "â€¢ ALL MLM business operations (membership, commissions, violations, status, regulations)\n"
    "â€¢ Product inquiries (pricing, availability, ordering, shipping)\n"
    "â€¢ General questions about what products do (not specialized medical/technical advice)\n"
    "â€¢ Returns, refunds, invoicing, training, events\n\n"
    
    "âœ… Always PRIVATE:\n"
    "â€¢ Technical agricultural advice (farming techniques, fertilizer types, pest control)\n"
    "â€¢ Medical/health advice (supplement interactions, dosage, conditions)\n"
    "â€¢ Specialized beauty/skincare advice (ingredient analysis, skin treatments)\n\n"
    
    "âœ… Always OFF_TOPIC:\n"
    "â€¢ Sports, entertainment, politics, general knowledge\n"
    "â€¢ Anything not related to PersianWay's business or product domains\n\n"
    
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
    
    "ðŸ“ EXAMPLES - Common Edge Cases:\n"
    "â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•\n\n"
    
    "Q: 'Ù…Ø­ØµÙˆÙ„ Ù‡Ù¾ÛŒÚ©Ø³ Ú†ÛŒÙ‡ØŸ' â†’ PUBLIC (product inquiry)\n"
    "Q: 'Ù‡Ù¾ÛŒÚ©Ø³ Ú†Ù†Ø¯ Ú©Ù¾Ø³ÙˆÙ„ Ø¨Ø®ÙˆØ±Ù…ØŸ' â†’ PRIVATE (dosage = medical advice)\n"
    "Q: 'Ù‚ÛŒÙ…Øª Ø³Ù„ÙˆÚ©Ø³ Ú†Ù†Ø¯Ù‡ØŸ' â†’ PUBLIC (pricing)\n"
    "Q: 'Ø³Ù„ÙˆÚ©Ø³ Ø¨Ø±Ø§ÛŒ Ù¾ÙˆØ³Øª Ú†Ø±Ø¨ Ø®ÙˆØ¨Ù‡ØŸ' â†’ PRIVATE (skincare advice)\n"
    "Q: 'Ø¨Ø±Ù†Ø¯Ù‡Ø§ÛŒ Ø¯ÛŒÚ¯Ù‡ Ú†ÛŒØ§ Ù‡Ø³ØªÙ†ØŸ' â†’ PUBLIC (company/brand info)\n"
    "Q: 'Ú©ÙˆØ¯ Ø§ÙˆØ±Ù‡ Ú†Ø·ÙˆØ± Ø§Ø³ØªÙØ§Ø¯Ù‡ Ú©Ù†Ù…ØŸ' â†’ PRIVATE (technical agriculture)\n"
    "Q: 'Ù¾ÙˆØ±Ø³Ø§Ù†Øª Ú©ÛŒ ÙˆØ§Ø±ÛŒØ² Ù…ÛŒØ´Ù‡ØŸ' â†’ PUBLIC (MLM business)\n"
    "Q: 'ØªØ®Ù„ÙØ§Øª Ú†Ù‡ Ù…Ø¬Ø§Ø²Ø§ØªÛŒ Ø¯Ø§Ø±Ù‡ØŸ' â†’ PUBLIC (MLM regulations)\n"
    "Q: 'ÙˆØ¶Ø¹ÛŒØª Ù†Ù…Ø§Ø¯ Ø²Ø±Ø¯ ÛŒØ¹Ù†ÛŒ Ú†ÛŒØŸ' â†’ PUBLIC (MLM status)\n\n"
    
    f"Conversation History:\n{history_block}\n\n"
    
    "Respond with valid JSON only:\n"
    "{\n"
    "  \"intent\": \"PUBLIC\" | \"PRIVATE\" | \"OFF_TOPIC\" | \"GREETING\" | \"FAREWELL\" | \"SMALL_TALK\",\n"
    "  \"category\": \"company_info\" | \"mlm_business\" | \"agriculture\" | \"health\" | \"beauty\" | \"unrelated\",\n"
    "  \"confidence\": 0.0-1.0,\n"
    "  \"explanation\": \"brief reason in English\",\n"
    "  \"off_topic_message\": \"optional: redirect message in Persian if OFF_TOPIC\"\n"
    "  \"greeting_message\": \"optional: greeting message and introduce yourself as Persianway AI Assistant in Persian if GREETING\"\n"
    "  \"farewell_message\": \"optional: polite closing in Persian if FAREWELL\"\n"
    "  \"small_talk_message\": \"optional: friendly short reply in Persian if SMALL_TALK\"\n"
    "}"
    )


        try:
            classifier_llm = llm or await get_llm(
                model_name="openai/gpt-4o-mini",
                temperature=0.0,
            )
        except Exception as e:
            logger.error(f"Failed to initialize intent detection LLM: {e}")
            return {
                "intent": "PRIVATE",
                "is_public": False,
                "explanation": f"Failed to initialize LLM: {str(e)}",
                "clarification_prompt": None
            }

        try:
            response = await classifier_llm.ainvoke([
                SystemMessage(content=classifier_prompt),
                HumanMessage(
                    content=(
                        f"Conversation history:\n{history_block}\n\n"
                        f"Latest user message:\n{message}\n\n"
                        "Classify the intent now."
                    )
                )
            ])
        except Exception as e:
            logger.error(f"Error during intent detection: {e}")
            return {
                "intent": "PRIVATE",
                "is_public": False,
                "explanation": f"Error during classification: {str(e)}",
                "clarification_prompt": None
            }

        content = (getattr(response, "content", "") or "").strip()
        if not content:
            logger.warning("Intent detection returned empty content")
            return {
                "intent": "PRIVATE",
                "is_public": False,
                "explanation": "Empty response from classifier",
                "clarification_prompt": None
            }

        # Parse JSON response
        payload = None
        json_match = re.search(r"\{.*\}", content, re.DOTALL)
        if json_match:
            try:
                payload = json.loads(json_match.group())
            except json.JSONDecodeError as e:
                logger.warning(f"Failed to parse JSON from intent detection: {e}")
                payload = None

        # Process the response
        if isinstance(payload, dict) and "intent" in payload:
            intent = payload.get("intent", "PRIVATE").upper()
            explanation = payload.get("explanation", "No explanation provided")
            clarification_prompt = payload.get("clarification_prompt")
            off_topic_message = payload.get("off_topic_message")
            greeting_message = payload.get("greeting_message")
            farewell_message = payload.get("farewell_message")
            small_talk_message = payload.get("small_talk_message")
  
            
            # Validate intent
            if intent not in ["PUBLIC", "PRIVATE", "OFF_TOPIC", "GREETING", "FAREWELL", "SMALL_TALK", "HELP_CAPABILITIES"]:
                logger.warning(f"Invalid intent '{intent}', defaulting to PRIVATE")
                intent = "PRIVATE"
            
            # Determine is_public for backward compatibility
            is_public = (intent == "PUBLIC")
            
            # Log the classification result
            logger.info(
                f"Intent classification: message='{message[:50]}...', "
                f"intent={intent}, is_public={is_public}, explanation='{explanation}'"
            )
            
            return {
                "intent": intent,
                "is_public": is_public,
                "explanation": explanation,
                "clarification_prompt": clarification_prompt,
                "off_topic_message": off_topic_message,
                "greeting_message": greeting_message,
                "farewell_message": farewell_message,
                "small_talk_message": small_talk_message,
            
            }

        # Fallback: try old format for backward compatibility
        if isinstance(payload, dict) and "public_data" in payload:
            value = payload.get("public_data")
            explanation = payload.get("explanation", "No explanation provided")
            
            is_public = False
            if isinstance(value, bool):
                is_public = value
            elif isinstance(value, str):
                normalized_value = value.strip().lower()
                is_public = normalized_value in {"true", "yes", "public", "public_data"}
            elif isinstance(value, (int, float)):
                is_public = bool(value)
            
            logger.info(f"Intent detection (legacy format): message='{message[:50]}...', is_public={is_public}")
            
            return {
                "intent": "PUBLIC" if is_public else "PRIVATE",
                "is_public": is_public,
                "explanation": explanation,
                "clarification_prompt": None
            }

        # Ultimate fallback
        logger.warning(f"Intent detection failed to classify message: '{message[:50]}...', defaulting to PRIVATE")
        return {
            "intent": "PRIVATE",
            "is_public": False,
            "explanation": "Failed to parse classifier response",
            "clarification_prompt": None
        }

    async def process_message(self, user_id: str, message: str, conversation_history: List = None, model: str = None, parameters: dict = None) -> Dict[str, Any]:
        """Process a user message using a hybrid approach.

        This service implements a three-tier approach:
        1. Check knowledge base for high-confidence answers
        2. Use general knowledge for domain-related topics with low KB confidence
        3. Refer unrelated topics to humans

        Args:
            user_id: Unique identifier for the user session
            message: The message from the user
            conversation_history: Previous conversation messages for context
            model: The model to use for processing
            parameters: Additional parameters for the model

        Returns:
            A dictionary representing the ChatResponse schema.
        """
        # We'll try to use the knowledge base regardless of which API key is configured
        # The document_processor will handle the availability of embeddings
        # This allows the system to work with either OpenAI or OpenRouter as the model provider
        # while still using OpenAI embeddings for the knowledge base if available

        
        # Get dynamic configuration
        await self.config_service._load_config()
        llm_settings = await self.config_service.get_llm_settings()
        rag_settings = await self.config_service.get_rag_settings()
        HUMAN_REFERRAL_MESSAGE = rag_settings.human_referral_message
        KB_CONFIDENCE_THRESHOLD = rag_settings.knowledge_base_confidence_threshold
        HISTORY=conversation_history
        query_analysis = {
            "confidence_score": 0.0,
            "knowledge_source": "none",
            "requires_human_referral": False,
            "reasoning": ""
        }
        params = parameters or {}
        response_parameters = {
            "model": model or llm_settings.default_model,
            "temperature": params.get("temperature", llm_settings.temperature),
            "max_tokens": params.get("max_tokens", llm_settings.max_tokens),
            "top_p": params.get("top_p", llm_settings.top_p)
        }
        answer = ""
    

        try:
            # First, check if the topic is related to our domain
             # Check if the topic is related to the domain
            # is_domain_related, unrelated_keyword = self._is_topic_related_to_domain(message)
            is_domain_related = True
            # is_domain_related=True
            if not is_domain_related:
                # Unrelated topic - refer to human
                answer = HUMAN_REFERRAL_MESSAGE
                query_analysis["confidence_score"] = 0.0
                query_analysis["knowledge_source"] = "none"
                query_analysis["requires_human_referral"] = True
                query_analysis["reasoning"] = f"Query is outside our domain expertise because it contains the keyword '{unrelated_keyword}', and requires human specialist attention."
            else:
                await self._ensure_latest_config()
                # Domain-related topic - first check intent
                intent_result = await self.detect_query_intent(message, conversation_history)
                
                # Handle off-topic questions
                if intent_result["intent"] == "OFF_TOPIC":
                    off_topic_msg = intent_result.get("off_topic_message") or (
                        "Ø¯Ø±ÙˆØ¯! ðŸŒ¹\n\n"
                        "Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø§ÛŒÙ† Ø³ÙˆØ§Ù„ Ø®Ø§Ø±Ø¬ Ø§Ø² Ø­ÙˆØ²Ù‡ ØªØ®ØµØµ Ù…Ø§Ø³Øª. Ù¾Ø±Ø´ÛŒÙ† ÙˆÛŒ Ø¯Ø± Ø­ÙˆØ²Ù‡â€ŒÙ‡Ø§ÛŒ Ø²ÛŒØ± Ø¢Ù…Ø§Ø¯Ù‡ Ú©Ù…Ú© Ø¨Ù‡ Ø´Ù…Ø§Ø³Øª:\n\n"
                        "ðŸŒ± **Ú©Ø´Ø§ÙˆØ±Ø²ÛŒ**: Ú©Ø§Ø´ØªØŒ Ø¯Ø§Ø´ØªØŒ Ú©ÙˆØ¯ØŒ Ø¢Ø¨ÛŒØ§Ø±ÛŒØŒ Ù…Ø¨Ø§Ø±Ø²Ù‡ Ø¨Ø§ Ø¢ÙØ§Øª\n"
                        "ðŸ’Š **Ø³Ù„Ø§Ù…Øª**: ØªØºØ°ÛŒÙ‡ØŒ ÙˆÛŒØªØ§Ù…ÛŒÙ†â€ŒÙ‡Ø§ØŒ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø³Ù„Ø§Ù…ØªÛŒ\n"
                        "ðŸ’„ **Ø²ÛŒØ¨Ø§ÛŒÛŒ**: Ù…Ø±Ø§Ù‚Ø¨Øª Ø§Ø² Ù¾ÙˆØ³ØªØŒ Ù…Ø­ØµÙˆÙ„Ø§Øª Ø¢Ø±Ø§ÛŒØ´ÛŒ Ùˆ Ø¨Ù‡Ø¯Ø§Ø´ØªÛŒ\n"
                        "ðŸ¢ **Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø±Ú©Øª**: Ø¯Ø±Ø¨Ø§Ø±Ù‡ Ù¾Ø±Ø´ÛŒÙ† ÙˆÛŒØŒ Ø®Ø¯Ù…Ø§Øª Ùˆ Ù…Ø­ØµÙˆÙ„Ø§Øª\n\n"
                        "Ú†Ø·ÙˆØ± Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ø¯Ø± Ø§ÛŒÙ† Ø²Ù…ÛŒÙ†Ù‡â€ŒÙ‡Ø§ Ø¨Ù‡ØªÙˆÙ† Ú©Ù…Ú© Ú©Ù†Ù…ØŸ"
                    )
                    
                    answer = off_topic_msg
                    query_analysis["confidence_score"] = 0.3
                    query_analysis["knowledge_source"] = "off_topic_redirect"
                    query_analysis["requires_human_referral"] = True
                    query_analysis["reasoning"] = f"Query is off-topic: {intent_result['explanation']}"
                    
                    # Add to conversation memory
                    conversation = await self._get_or_create_session(user_id, model, parameters)
                    conversation.memory.chat_memory.add_user_message(message)
                    conversation.memory.chat_memory.add_ai_message(answer)
                    
                    return {
                        "query_analysis": query_analysis,
                        "response_parameters": response_parameters,
                        "answer": answer
                    }
                if intent_result["intent"] == "GREETING":
                    answer =  intent_result.get("greeting_message") or (
                        "Ø¯Ø±ÙˆØ¯! Ø®ÙˆØ´ Ø¢Ù…Ø¯ÛŒØ¯ Ø¨Ù‡ Ù¾Ø±Ø´ÛŒÙ† ÙˆÛŒ ðŸŒ·\n\n"
                        "Ù…ÛŒâ€ŒØªÙˆÙ†ÛŒØ¯ Ø¯Ø± Ø§ÛŒÙ† Ø²Ù…ÛŒÙ†Ù‡â€ŒÙ‡Ø§ Ø³ÙˆØ§Ù„ Ø¨Ù¾Ø±Ø³ÛŒØ¯:\n\n"
                        "ðŸŒ± Ú©Ø´Ø§ÙˆØ±Ø²ÛŒ: Ú©Ø§Ø´ØªØŒ Ø¯Ø§Ø´ØªØŒ Ú©ÙˆØ¯Ø¯Ù‡ÛŒØŒ Ø¢Ø¨ÛŒØ§Ø±ÛŒØŒ Ú©Ù†ØªØ±Ù„ Ø¢ÙØ§Øª\n"
                        "ðŸ’Š Ø³Ù„Ø§Ù…Øª: Ù…Ú©Ù…Ù„â€ŒÙ‡Ø§ØŒ ØªØ¯Ø§Ø®Ù„â€ŒÙ‡Ø§ØŒ Ø¯ÙˆØ² Ù…ØµØ±ÙØŒ ØªØºØ°ÛŒÙ‡\n"
                        "ðŸ’„ Ø²ÛŒØ¨Ø§ÛŒÛŒ: Ù…Ø±Ø§Ù‚Ø¨Øª Ø§Ø² Ù¾ÙˆØ³ØªØŒ ØªØ±Ú©ÛŒØ¨Ø§ØªØŒ Ø±ÙˆØªÛŒÙ†â€ŒÙ‡Ø§\n"
                        "ðŸ¢ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø±Ú©Øª: Ø«Ø¨Øªâ€ŒÙ†Ø§Ù…ØŒ Ù¾ÙˆØ±Ø³Ø§Ù†ØªØŒ Ù‚ÙˆØ§Ù†ÛŒÙ†ØŒ Ø³ÙØ§Ø±Ø´ Ùˆ Ø§Ø±Ø³Ø§Ù„\n\n"
                        "Ù‡Ø± Ø³ÙˆØ§Ù„ÛŒ Ø¯Ø§Ø±ÛŒØ¯ Ø¨ÙØ±Ù…Ø§ÛŒÛŒØ¯Ø› Ø¨Ø§ Ú©Ù…Ø§Ù„ Ù…ÛŒÙ„ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ù…ÛŒâ€ŒÚ©Ù†Ù…."
                    )
                    query_analysis["confidence_score"] = 0.5
                    query_analysis["knowledge_source"] = "greeting"
                    query_analysis["requires_human_referral"] = False
                    query_analysis["reasoning"] = "User initiated conversation with a greeting."
                    response_parameters["temperature"] = 0.2
                    conversation = await self._get_or_create_session(user_id, model, parameters)
                    conversation.memory.chat_memory.add_user_message(message)
                    conversation.memory.chat_memory.add_ai_message(answer)
                    return {
                        "query_analysis": query_analysis,
                        "response_parameters": response_parameters,
                        "answer": answer
                    }
                if intent_result["intent"] == "FAREWELL":
                    answer = intent_result.get("farewell_message") or (
                        "Ø³Ù¾Ø§Ø³ Ø§Ø² Ù‡Ù…Ø±Ø§Ù‡ÛŒ Ø´Ù…Ø§ ðŸŒŸ\nØ§Ú¯Ø± Ø³ÙˆØ§Ù„ Ø¯ÛŒÚ¯Ø±ÛŒ Ø¯Ø§Ø±ÛŒØ¯ Ø¯Ø± Ù‡Ø± Ø²Ù…Ø§Ù† Ø®ÙˆØ´Ø­Ø§Ù„ Ù…ÛŒâ€ŒØ´ÙˆÙ… Ú©Ù…Ú© Ú©Ù†Ù…. Ø±ÙˆØ²ØªÙˆÙ† Ø¨Ø®ÛŒØ±!"
                    )
                    query_analysis["confidence_score"] = 0.5
                    query_analysis["knowledge_source"] = "farewell"
                    query_analysis["requires_human_referral"] = False
                    query_analysis["reasoning"] = "User ended the conversation."
                    response_parameters["temperature"] = 0.2
                    conversation = await self._get_or_create_session(user_id, model, parameters)
                    conversation.memory.chat_memory.add_user_message(message)
                    conversation.memory.chat_memory.add_ai_message(answer)
                    return {
                        "query_analysis": query_analysis,
                        "response_parameters": response_parameters,
                        "answer": answer
                    }
                if intent_result["intent"] == "SMALL_TALK":
                    answer = intent_result.get("small_talk_message") or (
                        "Ø±ÙˆØ² Ø´Ù…Ø§ Ù‡Ù… Ø¨Ø®ÛŒØ± ðŸ˜Š\nØ¯Ø± Ú†Ù‡ Ø²Ù…ÛŒÙ†Ù‡â€ŒØ§ÛŒ Ù…ÛŒâ€ŒØªÙˆÙ†Ù… Ú©Ù…Ú© Ú©Ù†Ù…ØŸ Ú©Ø´Ø§ÙˆØ±Ø²ÛŒØŒ Ø³Ù„Ø§Ù…ØªØŒ Ø²ÛŒØ¨Ø§ÛŒÛŒ ÛŒØ§ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø±Ú©ØªØŸ"
                    )
                    query_analysis["confidence_score"] = 0.5
                    query_analysis["knowledge_source"] = "small_talk"
                    query_analysis["requires_human_referral"] = False
                    query_analysis["reasoning"] = "User engaged in small talk."
                    response_parameters["temperature"] = 0.2
                    conversation = await self._get_or_create_session(user_id, model, parameters)
                    conversation.memory.chat_memory.add_user_message(message)
                    conversation.memory.chat_memory.add_ai_message(answer)
                    return {
                        "query_analysis": query_analysis,
                        "response_parameters": response_parameters,
                        "answer": answer
                    }
                if intent_result["intent"] == "HELP_CAPABILITIES":
                    answer =  (
                        "Ù…Ù† Ø¯Ø³ØªÛŒØ§Ø± Ù‡ÙˆØ´Ù…Ù†Ø¯ Ù¾Ø±Ø´ÛŒÙ† ÙˆÛŒ Ù‡Ø³ØªÙ… ðŸ¤–\nÙ…ÛŒâ€ŒØªÙˆÙ†Ù… Ø¯Ø± Ø§ÛŒÙ† Ø­ÙˆØ²Ù‡â€ŒÙ‡Ø§ Ú©Ù…Ú© Ú©Ù†Ù…:\n\n"
                        "ðŸŒ± Ú©Ø´Ø§ÙˆØ±Ø²ÛŒ: Ú©ÙˆØ¯Ø¯Ù‡ÛŒØŒ Ø¢Ø¨ÛŒØ§Ø±ÛŒØŒ Ø¢ÙØ§ØªØŒ Ø±ÙˆØ´â€ŒÙ‡Ø§ÛŒ Ú©Ø´Øª\n"
                        "ðŸ’Š Ø³Ù„Ø§Ù…Øª: Ø¯ÙˆØ² Ù…Ú©Ù…Ù„â€ŒÙ‡Ø§ØŒ ØªØ¯Ø§Ø®Ù„â€ŒÙ‡Ø§ØŒ ØªØºØ°ÛŒÙ‡ Ø¹Ù„Ù…ÛŒ\n"
                        "ðŸ’„ Ø²ÛŒØ¨Ø§ÛŒÛŒ: Ø±ÙˆØªÛŒÙ†â€ŒÙ‡Ø§ØŒ ØªØ±Ú©ÛŒØ¨Ø§ØªØŒ Ø¯Ø±Ù…Ø§Ù†â€ŒÙ‡Ø§ÛŒ Ù¾ÙˆØ³ØªÛŒ\n"
                        "ðŸ¢ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ø´Ø±Ú©Øª: Ø«Ø¨Øªâ€ŒÙ†Ø§Ù…ØŒ Ù¾ÙˆØ±Ø³Ø§Ù†ØªØŒ Ù‚ÙˆØ§Ù†ÛŒÙ†ØŒ Ø³ÙØ§Ø±Ø´\n\n"
                        "Ú©Ø§ÙÛŒÙ‡ Ø³ÙˆØ§Ù„ØªÙˆÙ† Ø±Ùˆ Ù‡Ù…ÛŒÙ†â€ŒØ¬Ø§ Ø¨Ù¾Ø±Ø³ÛŒØ¯ ØªØ§ Ø±Ø§Ù‡Ù†Ù…Ø§ÛŒÛŒ Ú©Ù†Ù…."
                    )
                    query_analysis["confidence_score"] = 0.6
                    query_analysis["knowledge_source"] = "help_capabilities"
                    query_analysis["requires_human_referral"] = False
                    query_analysis["reasoning"] = "User asked for assistant capabilities."
                    response_parameters["temperature"] = 0.2
                    conversation = await self._get_or_create_session(user_id, model, parameters)
                    conversation.memory.chat_memory.add_user_message(message)
                    conversation.memory.chat_memory.add_ai_message(answer)
                    return {
                        "query_analysis": query_analysis,
                        "response_parameters": response_parameters,
                        "answer": answer
                    }
                
           
                # Proceed with knowledge base query
                is_public = intent_result["is_public"]
                try:
                    kb_service = get_knowledge_base_service()
                    kb_result = await kb_service.query_knowledge_base(message, conversation_history, is_public)
                    kb_confidence = kb_result.get("confidence_score", 0) if kb_result else 0
                    logger.debug(f"[DEBUG] KB raw confidence: {kb_confidence:.3f}")
                except RuntimeError as kb_error:
                    # Vector store configuration error - log and inform user
                    logging.error(f"Knowledge base configuration error: {str(kb_error)}")
                    answer = f"Ø®Ø·Ø§ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ÛŒ Ø¨Ù‡ Ù¾Ø§ÛŒÚ¯Ø§Ù‡ Ø¯Ø§Ù†Ø´: {str(kb_error)}"
                    query_analysis["confidence_score"] = 0.0
                    query_analysis["knowledge_source"] = "error"
                    query_analysis["requires_human_referral"] = True
                    query_analysis["reasoning"] = "Knowledge base configuration error - vector store not available."
                    return {
                        "query_analysis": query_analysis,
                        "response_parameters": response_parameters,
                        "answer": answer
                    }
                except Exception as kb_error:
                    # Other knowledge base errors - fall back to general knowledge
                    logging.warning(f"Knowledge base query failed: {str(kb_error)}")
                    kb_confidence = 0  # Set to 0 to trigger general knowledge fallback

                # Define referral indicators once for reuse
                referral_indicators = [
                    "Ù…ØªØ§Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª",
                    "Ù…ØªØ£Ø³ÙØ§Ù†Ù‡ Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ",
                    "Ù…ØªØ£Ø³ÙØ§Ù†Ù‡",
                    "Ù…ØªØ§Ø³ÙØ§Ù†Ù‡",
                    "Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ø¯Ø± Ø¯Ø³ØªØ±Ø³ Ù†ÛŒØ³Øª",
                    "Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ø¯Ø±Ø¨Ø§Ø±Ù‡",
                    "Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ø¯Ø± Ù…ÙˆØ±Ø¯",
                    "Ø§Ø·Ù„Ø§Ø¹Ø§Øª Ú©Ø§ÙÛŒ Ø¨Ø±Ø§ÛŒ Ù¾Ø§Ø³Ø®",
                ]
                
                if kb_confidence >= KB_CONFIDENCE_THRESHOLD:
                    # High confidence answer from knowledge base - priority source
                    answer = kb_result["answer"]
                    
                    # Check for referral indicators even with high confidence
                    if any(indicator in answer for indicator in referral_indicators):
                        answer = answer
                        query_analysis["requires_human_referral"] = True
                        query_analysis["reasoning"] = "Knowledge base answer contains referral indicators despite high confidence."
                    else:
                        query_analysis["confidence_score"] = kb_confidence
                        query_analysis["knowledge_source"] = kb_result.get("source_type", "knowledge_base")
                        query_analysis["requires_human_referral"] = False
                        query_analysis["reasoning"] = "High confidence answer found in knowledge base (priority source)."
                    
                    response_parameters["temperature"] = 0.1  # Low temperature for factual answers
                        
                else:
                    # Low KB confidence - check if general answers are allowed
                    if self.generalAnswer:
                        # Domain-related but low confidence - try general knowledge as fallback
                        conversation = await self._get_or_create_session(user_id, model, parameters)
                        
                        # Get response using general knowledge. The conversation object already has the system prompt.
                        response = conversation.predict(input=message)
                        
                        # Check if the model indicated it needs human referral
                        if any(indicator in response for indicator in referral_indicators):
                            answer = answer
                            query_analysis["requires_human_referral"] = True
                            query_analysis["reasoning"] = "Model determined the query requires specialist attention."
                        else:
                            answer = response
                            query_analysis["confidence_score"] = 0.6  # Assign a default confidence for general knowledge
                            query_analysis["knowledge_source"] = "general_knowledge"
                            query_analysis["requires_human_referral"] = False
                            query_analysis["reasoning"] = "Answer provided from general knowledge."
                            response_parameters["temperature"] = 0.3  # Moderate temperature for general knowledge
                    else:
                        # General answers are disabled - refer to human
                        answer = HUMAN_REFERRAL_MESSAGE
                        query_analysis["requires_human_referral"] = True
                        query_analysis["reasoning"] = "Low knowledge base confidence and general answers are disabled."

            # Add the final interaction to the conversation history.
            # The `conversation.predict` call above already adds the user message and the AI response to the memory
            # for the general_knowledge case. We need to manually add it for other cases.
            if query_analysis["knowledge_source"] != "general_knowledge":
                conversation = await self._get_or_create_session(user_id, model, parameters)
                conversation.memory.chat_memory.add_user_message(message)
                conversation.memory.chat_memory.add_ai_message(answer)

            # Construct the final response dictionary
            logger.debug(f"[DEBUG] Final confidence: {query_analysis['confidence_score']:.3f}")
            return {
                "query_analysis": query_analysis,
                "response_parameters": response_parameters,
                "answer": answer
            }

        except Exception as e:
            error_msg = f"Error processing message: {str(e)}"
            # Fallback to human referral on any processing error
            query_analysis["requires_human_referral"] = True
            query_analysis["reasoning"] = f"An internal error occurred: {error_msg}"
            query_analysis["confidence_score"] = 0.0
            query_analysis["knowledge_source"] = "none"
            return {
                "query_analysis": query_analysis,
                "response_parameters": response_parameters,
                "answer": HUMAN_REFERRAL_MESSAGE
            }
    

    
    def get_conversation_history(self, user_id: str) -> Optional[List[ChatMessage]]:
        """Get the conversation history for a user.
        
        Args:
            user_id: Unique identifier for the user session
            
        Returns:
            A list of ChatMessage objects or None if no history exists
        """
        if user_id not in self._memories:
            return None
        
        memory = self._memories[user_id]
        history = []
        
        # Convert LangChain memory to our ChatMessage schema
        # Filter out SystemMessage to avoid including system prompts in conversation history
        for message in memory.chat_memory.messages:
            if isinstance(message, HumanMessage):
                history.append(ChatMessage(role="user", content=message.content))
            elif isinstance(message, AIMessage):
                history.append(ChatMessage(role="assistant", content=message.content))
            # SystemMessage is intentionally excluded from conversation history
        
        return history


# Singleton instance
_chat_service = None


def get_chat_service() -> ChatService:
    """Get the chat service instance.
    
    Returns:
        A singleton instance of the ChatService
    """
    global _chat_service
    if _chat_service is None:
        _chat_service = ChatService()
    return _chat_service
